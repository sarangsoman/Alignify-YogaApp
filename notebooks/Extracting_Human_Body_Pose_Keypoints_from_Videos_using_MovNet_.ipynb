{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Welcome to my Colab notebook that uses computer vision techniques to extract key information from videos. In this notebook, we will reduce the resolution of a video and convert it into frames. Then, we will pass each frame through a MovNet model to generate 17 keypoints (x, y, confidence) representing the body pose in the frame. We will then save the results from all the frames in a numpy array. This notebook is a powerful tool for analyzing human movements in videos and can be used in a variety of applications, such as sports analysis, motion capture, and more.*"
      ],
      "metadata": {
        "id": "lj4APKU2Nor0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcR5VCDKdjo0",
        "outputId": "418dbe03-ec86-4f4f-c9e2-8f831e5db6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o37zeSPDdO8y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "import pandas as pd\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython.display import HTML, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage.transform import resize\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_video(video):\n",
        "    fig = plt.figure(figsize=(15,15))  # Display size specification\n",
        "\n",
        "    mov = []\n",
        "    for i in range(len(video)):  # Append videos one by one to mov\n",
        "        img = plt.imshow(cv2.cvtColor(video[i], cv2.COLOR_BGR2RGB), animated=True)\n",
        "        plt.axis('off')\n",
        "        mov.append([img])\n",
        "\n",
        "    # Animation creation\n",
        "    anime = animation.ArtistAnimation(fig, mov, interval=50, repeat_delay=1000)\n",
        "\n",
        "    plt.close()\n",
        "    return anime"
      ],
      "metadata": {
        "id": "ikZi7RSJRJIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducing Video Resolution"
      ],
      "metadata": {
        "id": "TEFTzY9CJJv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Load the input video\n",
        "cap = cv2.VideoCapture('PullUP_F.mp4')\n",
        "\n",
        "# Set the output video codec\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "# Set the output video dimensions and frame rate\n",
        "width, height = 256, 256\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Create the output video writer\n",
        "out = cv2.VideoWriter('pullUP_F256.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "# Process each frame of the input video\n",
        "while True:\n",
        "    # Read a frame from the input video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # If the frame cannot be read, break the loop\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Resize the frame to the output dimensions\n",
        "    resized_frame = cv2.resize(frame, (width, height))\n",
        "\n",
        "    # Write the resized frame to the output video\n",
        "    out.write(resized_frame)\n",
        "\n",
        "# Release the input and output video objects\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "id": "vKGfpt2HQXAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading MoveNet: Ultra fast and accurate pose detection model."
      ],
      "metadata": {
        "id": "44sseT4deAsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[MoveNet](https://t.co/QpfnVL0YYI?amp=1)** is an ultra fast and accurate model that detects 17 keypoints of a body. The model is offered on [TF Hub](https://tfhub.dev/s?q=movenet) with two variants, known as Lightning and Thunder. Lightning is intended for latency-critical applications, while Thunder is intended for applications that require high accuracy. Both models run faster than real time (30+ FPS) on most modern desktops, laptops, and phones, which proves crucial for live fitness, health, and wellness applications.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/combined_squat_dance.gif\" alt=\"drawing\"/>\n"
      ],
      "metadata": {
        "id": "g58Y55ODL0e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"movenet_thunder\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
        "\n",
        "if \"tflite\" in model_name:\n",
        "  if \"movenet_lightning_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  elif \"movenet_lightning_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    # TF Lite format expects tensor type of uint8.\n",
        "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "    # Invoke inference.\n",
        "    interpreter.invoke()\n",
        "    # Get the model prediction.\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return keypoints_with_scores\n",
        "\n",
        "else:\n",
        "  if \"movenet_lightning\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    model = module.signatures['serving_default']\n",
        "\n",
        "    # SavedModel format expects tensor type of int32.\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    # Run model inference.\n",
        "    outputs = model(input_image)\n",
        "    # Output is a [1, 1, 17, 3] tensor.\n",
        "    keypoints_with_scores = outputs['output_0'].numpy()\n",
        "    return keypoints_with_scores"
      ],
      "metadata": {
        "id": "PL2U2H9Td9Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# helper functions"
      ],
      "metadata": {
        "id": "KV9e2xblfgEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cropping Algorithm\n",
        "\n",
        "# Confidence score to determine whether a keypoint prediction is reliable.\n",
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "  \"\"\"Defines the default crop region.\n",
        "\n",
        "  The function provides the initial crop region (pads the full image from both\n",
        "  sides to make it a square image) when the algorithm cannot reliably determine\n",
        "  the crop region from the previous frame.\n",
        "  \"\"\"\n",
        "  if image_width > image_height:\n",
        "    box_height = image_width / image_height\n",
        "    box_width = 1.0\n",
        "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "    x_min = 0.0\n",
        "  else:\n",
        "    box_height = 1.0\n",
        "    box_width = image_height / image_width\n",
        "    y_min = 0.0\n",
        "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "  return {\n",
        "    'y_min': y_min,\n",
        "    'x_min': x_min,\n",
        "    'y_max': y_min + box_height,\n",
        "    'x_max': x_min + box_width,\n",
        "    'height': box_height,\n",
        "    'width': box_width\n",
        "  }\n",
        "\n",
        "def torso_visible(keypoints):\n",
        "  \"\"\"Checks whether there are enough torso keypoints.\n",
        "\n",
        "  This function checks whether the model is confident at predicting one of the\n",
        "  shoulders/hips which is required to determine a good crop region.\n",
        "  \"\"\"\n",
        "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE) and\n",
        "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE))\n",
        "\n",
        "def determine_torso_and_body_range(\n",
        "    keypoints, target_keypoints, center_y, center_x):\n",
        "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
        "\n",
        "  The function returns the maximum distances from the two sets of keypoints:\n",
        "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
        "  used to determine the crop size. See determineCropRegion for more detail.\n",
        "  \"\"\"\n",
        "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "  max_torso_yrange = 0.0\n",
        "  max_torso_xrange = 0.0\n",
        "  for joint in torso_joints:\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "    if dist_y > max_torso_yrange:\n",
        "      max_torso_yrange = dist_y\n",
        "    if dist_x > max_torso_xrange:\n",
        "      max_torso_xrange = dist_x\n",
        "\n",
        "  max_body_yrange = 0.0\n",
        "  max_body_xrange = 0.0\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "      continue\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
        "    if dist_y > max_body_yrange:\n",
        "      max_body_yrange = dist_y\n",
        "\n",
        "    if dist_x > max_body_xrange:\n",
        "      max_body_xrange = dist_x\n",
        "\n",
        "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
        "\n",
        "def determine_crop_region(\n",
        "      keypoints, image_height,\n",
        "      image_width):\n",
        "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
        "\n",
        "  The algorithm uses the detected joints from the previous frame to estimate\n",
        "  the square region that encloses the full body of the target person and\n",
        "  centers at the midpoint of two hip joints. The crop size is determined by\n",
        "  the distances between each joints and the center point.\n",
        "  When the model is not confident with the four torso joint predictions, the\n",
        "  function returns a default crop which is the full image padded to square.\n",
        "  \"\"\"\n",
        "  target_keypoints = {}\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    target_keypoints[joint] = [\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "    ]\n",
        "\n",
        "  if torso_visible(keypoints):\n",
        "    center_y = (target_keypoints['left_hip'][0] +\n",
        "                target_keypoints['right_hip'][0]) / 2;\n",
        "    center_x = (target_keypoints['left_hip'][1] +\n",
        "                target_keypoints['right_hip'][1]) / 2;\n",
        "\n",
        "    (max_torso_yrange, max_torso_xrange,\n",
        "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "          keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "    crop_length_half = np.amax(\n",
        "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "    tmp = np.array(\n",
        "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "    crop_length_half = np.amin(\n",
        "        [crop_length_half, np.amax(tmp)]);\n",
        "\n",
        "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
        "\n",
        "    if crop_length_half > max(image_width, image_height) / 2:\n",
        "      return init_crop_region(image_height, image_width)\n",
        "    else:\n",
        "      crop_length = crop_length_half * 2;\n",
        "      return {\n",
        "        'y_min': crop_corner[0] / image_height,\n",
        "        'x_min': crop_corner[1] / image_width,\n",
        "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "        'height': (crop_corner[0] + crop_length) / image_height -\n",
        "            crop_corner[0] / image_height,\n",
        "        'width': (crop_corner[1] + crop_length) / image_width -\n",
        "            crop_corner[1] / image_width\n",
        "      }\n",
        "  else:\n",
        "    return init_crop_region(image_height, image_width)\n",
        "\n",
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
        "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
        "          crop_region['y_max'], crop_region['x_max']]]\n",
        "  output_image = tf.image.crop_and_resize(\n",
        "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "  return output_image\n",
        "\n",
        "def run_inference(movenet, image, crop_region, crop_size):\n",
        "  \"\"\"Runs model inferece on the cropped region.\n",
        "\n",
        "  The function runs the model inference on the cropped region and updates the\n",
        "  model output to the original image coordinate system.\n",
        "  \"\"\"\n",
        "  image_height, image_width, _ = image.shape\n",
        "  input_image = crop_and_resize(\n",
        "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "  # Run model inference.\n",
        "  keypoints_with_scores = movenet(input_image)\n",
        "  # Update the coordinates.\n",
        "  for idx in range(17):\n",
        "    keypoints_with_scores[0, 0, idx, 0] = (\n",
        "        crop_region['y_min'] * image_height +\n",
        "        crop_region['height'] * image_height *\n",
        "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "    keypoints_with_scores[0, 0, idx, 1] = (\n",
        "        crop_region['x_min'] * image_width +\n",
        "        crop_region['width'] * image_width *\n",
        "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "  return keypoints_with_scores\n",
        "def progress(value, max=100):\n",
        "  return HTML(\"\"\"\n",
        "      <progress\n",
        "          value='{value}'\n",
        "          max='{max}',\n",
        "          style='width: 100%'\n",
        "      >\n",
        "          {value}\n",
        "      </progress>\n",
        "  \"\"\".format(value=value, max=max))"
      ],
      "metadata": {
        "id": "z6BbybzGffuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions for visualization\n",
        "\n",
        "# Dictionary that maps from joint names to keypoint indices.\n",
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "}\n",
        "\n",
        "# Maps bones to a matplotlib color name.\n",
        "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}"
      ],
      "metadata": {
        "id": "Lf2qmtsFkoBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# converting the video to frames"
      ],
      "metadata": {
        "id": "UfEdzozfeNRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np_config.enable_numpy_behavior()\n",
        "def loadVid(path):\n",
        "    # Create a VideoCapture object and read from input file\n",
        "    # If the input is the camera, pass 0 instead of the video file name\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    # Check if camera opened successfully\n",
        "    if (cap.isOpened()== False):\n",
        "        print(\"Error opening video stream or file\")\n",
        "\n",
        "    i = 0\n",
        "    # Read until video is completed\n",
        "    while(cap.isOpened()):\n",
        "        # Capture frame-by-frame\n",
        "        i += 1\n",
        "        ret, frame = cap.read()\n",
        "        if ret == True:\n",
        "\n",
        "            #Store the resulting frame\n",
        "            if i == 1:\n",
        "                frames = frame[np.newaxis, ...]\n",
        "            else:\n",
        "                frame = frame[np.newaxis, ...]\n",
        "                frames = np.vstack([frames, frame])\n",
        "                frames = np.squeeze(frames)\n",
        "\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # When everything done, release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    return frames"
      ],
      "metadata": {
        "id": "O1UfR5vBeKDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading the video"
      ],
      "metadata": {
        "id": "Xvbj8JtoeqWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames=loadVid(\"pullUP_F256.mp4\")\n",
        "frames=tf.convert_to_tensor(frames)"
      ],
      "metadata": {
        "id": "qDqQI7iDr88B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "yJ6mTTuzKUDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(frames):\n",
        "\n",
        "  rotated_90 = tf.image.rot90(frames)\n",
        "  frames = tf.concat([frames, rotated_90], axis=0)\n",
        "  del rotated_90\n",
        "  flipped_images = tf.image.flip_left_right(frames)\n",
        "  frames = tf.concat([frames, flipped_images], axis=0)\n",
        "  del flipped_images\n",
        "  return frames"
      ],
      "metadata": {
        "id": "ydo860uIr79h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyTIU4igN8Ot",
        "outputId": "c596436d-9d03-43d9-bfd9-c11b88c78bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5447, 256, 256, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flipped_images=tf.image.flip_left_right(frames)"
      ],
      "metadata": {
        "id": "jJzBcENPKx8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = tf.concat([frames, flipped_images], axis=0)"
      ],
      "metadata": {
        "id": "a1ZdP2soa3aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotated_90 = tf.image.rot90(frames[:2000])\n",
        "frames = tf.concat([frames, rotated_90], axis=0)"
      ],
      "metadata": {
        "id": "w4Hg5cTJU7d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhHmqykkOEud",
        "outputId": "efcaca6a-ce70-42f3-d97a-7f7a76c50442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([12894, 256, 256, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating the keypoints"
      ],
      "metadata": {
        "id": "tO3PC0lVLBek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_frames, image_height, image_width, _ = frames.shape\n",
        "crop_region = init_crop_region(image_height, image_width)\n",
        "\n",
        "output_keypoints=[]\n",
        "bar = display(progress(0, num_frames-1), display_id=True)\n",
        "for frame_idx in range(num_frames):\n",
        "  keypoints_with_scores = run_inference(\n",
        "        movenet, frames[frame_idx, :, :, :], crop_region,\n",
        "        crop_size=[input_size, input_size])\n",
        "\n",
        "  output_keypoints.append(keypoints_with_scores)\n",
        "\n",
        "  crop_region = determine_crop_region(\n",
        "      keypoints_with_scores, image_height, image_width)\n",
        "  bar.update(progress(frame_idx, num_frames-1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "werItaiTe63d",
        "outputId": "a79f435d-56f7-4bd0-a7a7-52e26de93521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <progress\n",
              "          value='12893'\n",
              "          max='12893',\n",
              "          style='width: 100%'\n",
              "      >\n",
              "          12893\n",
              "      </progress>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saving the data"
      ],
      "metadata": {
        "id": "AkLCfMrRhDKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_keypoints=np.array(output_keypoints)"
      ],
      "metadata": {
        "id": "eqx8B4AS0myD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('pullUP_F.npy', output_keypoints)\n"
      ],
      "metadata": {
        "id": "QqjYaco5lPGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_keypoints.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eHu-pQZ0bwu",
        "outputId": "4ad35175-974f-422c-a97b-6bba46198fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12894, 1, 1, 17, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_keypoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00W21ZaRN9Vy",
        "outputId": "7bad3508-2fd7-481a-8d0e-878efc911aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[[2.92048067e-01, 5.22662878e-01, 1.63227529e-03],\n",
              "          [3.94913673e-01, 5.01913488e-01, 1.96826435e-03],\n",
              "          [2.38119677e-01, 4.90857422e-01, 3.33389995e-04],\n",
              "          ...,\n",
              "          [5.61807513e-01, 2.85854071e-01, 1.84777826e-02],\n",
              "          [6.34234309e-01, 7.04065561e-01, 1.30680175e-02],\n",
              "          [6.71082735e-01, 2.76862174e-01, 2.23788973e-02]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[2.92048067e-01, 5.22662878e-01, 1.63227529e-03],\n",
              "          [3.94913673e-01, 5.01913488e-01, 1.96826435e-03],\n",
              "          [2.38119677e-01, 4.90857422e-01, 3.33389995e-04],\n",
              "          ...,\n",
              "          [5.61807513e-01, 2.85854071e-01, 1.84777826e-02],\n",
              "          [6.34234309e-01, 7.04065561e-01, 1.30680175e-02],\n",
              "          [6.71082735e-01, 2.76862174e-01, 2.23788973e-02]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[2.92048067e-01, 5.22662878e-01, 1.63227529e-03],\n",
              "          [3.94913673e-01, 5.01913488e-01, 1.96826435e-03],\n",
              "          [2.38119677e-01, 4.90857422e-01, 3.33389995e-04],\n",
              "          ...,\n",
              "          [5.61807513e-01, 2.85854071e-01, 1.84777826e-02],\n",
              "          [6.34234309e-01, 7.04065561e-01, 1.30680175e-02],\n",
              "          [6.71082735e-01, 2.76862174e-01, 2.23788973e-02]]]],\n",
              "\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "\n",
              "       [[[[4.04275864e-01, 3.23877573e-01, 3.89402241e-01],\n",
              "          [3.80223781e-01, 3.08069646e-01, 5.12389541e-01],\n",
              "          [3.92473042e-01, 3.22470784e-01, 4.35176760e-01],\n",
              "          ...,\n",
              "          [6.09507143e-01, 8.54452312e-01, 6.24060869e-01],\n",
              "          [3.89787316e-01, 1.18164636e-01, 3.12702924e-01],\n",
              "          [5.94798803e-01, 7.96394706e-01, 4.97342460e-02]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[3.98804784e-01, 3.25500667e-01, 4.40699399e-01],\n",
              "          [3.77842605e-01, 3.13418001e-01, 4.28768992e-01],\n",
              "          [3.89229625e-01, 3.21900129e-01, 4.48886395e-01],\n",
              "          ...,\n",
              "          [6.17924631e-01, 8.45811307e-01, 7.16056585e-01],\n",
              "          [3.87507915e-01, 1.17650755e-01, 2.68292964e-01],\n",
              "          [5.35457969e-01, 2.00611800e-01, 2.99425244e-01]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[3.99027467e-01, 3.21921945e-01, 4.11806285e-01],\n",
              "          [3.78660828e-01, 3.10248852e-01, 5.72195470e-01],\n",
              "          [3.90980482e-01, 3.19023848e-01, 4.25660729e-01],\n",
              "          ...,\n",
              "          [6.10627472e-01, 8.27060342e-01, 7.39089072e-01],\n",
              "          [8.23044240e-01, 9.85980153e-01, 2.65734881e-01],\n",
              "          [8.13671947e-01, 9.91119027e-01, 2.11519226e-01]]]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}